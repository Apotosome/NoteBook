   





# 循环神经网络

​	`循环神经网络RNN`是一类具有短期记忆能力的神经网络.在循环神经网中，神经元不但可以接受其他神经元的信息，也可以接受自身的信息，形成具有环路的网络结构.

​	以飞机订票系统中订票信息的槽位填充为例:词的表示方法有`1-of-N encoding`,`Beyond 1-of-N encoding`.后者是在前者的基础上添加了other维度.

​	按照解决多分类问题的思路使用`FNN`来解决槽位填充问题.输入：词向量,输出：该词属于某一槽位的概率.但是使用`FNN`有一定的局限性。比如目的地的关键词属于哪一个槽位还与其上下文有关，因此解决该问题的神经网络需要具有记忆。

​	给网络增加记忆的方法有三种：`延时神经网络TDNN`,`有外部输入的非线性自回归模型NARX`,`循环神经网络RNN`

​	循环神经网络具有短期记忆能力，因此其计算能力十分强大.根据通用近似定理，两层的`FNN`可以近似任意有界闭集上的任意连续函数.而循环神经网络可以近似任意的非线性动力系统.



​	`简单循环网络SRN`是一个只有一个隐藏层的神经网络,是在一个两层的`FNN`上添加了从隐藏层到隐藏层的反馈连接.
$$
h_t = f(Uh_{t-1}+Wx_t+b)
$$
​	需要注意在`SRN`中随机初始化参数时使用`sigmoid`激活函数会比使用`ReLU`效果更好一些，但是使用单位矩阵初始化参数时`ReLU`激活函数会比`sigmoid`激活函数效果要好。

​	

​	`RNN`可以应用到各种不同类型的机器学习任务,具体可以划分为:

1. 序列到类别模式

   输入为序列，输出为类别.应用实例：情感分析，关键词提取

2. 同步的序列到序列模式

   每一时刻的都有输入和输出，输入和输出序列长度相同。应用实例：词性标注

3. 异步的序列到序列模式.

   输入和输出序列没有严格的规定，不需要保持相同的长度。应用实例：语音识别，机器翻译

   

​	`RNN`的参数可以通过梯度下降方法来进行学习.在`RNN`中主要有两种计算梯度的方式:`随时间反向传播BPTT`和`实时循环学习RTRL`

- `BPTT`:通过类似`FNN`的误差反向传播算法计算梯度

  ![image-20211121221141416](/Users/jin/Library/Application Support/typora-user-images/image-20211121221141416.png)

- 一般网络输出维度远低于输入维度，因此` BPTT` 计算量会更小，但是 BPTT 算法需要保存所有时刻的中间梯度，空间复杂度较高.`RTRL `不需要梯度回传，因此非常适合用于需要在线学习或无限序列的任务中.

- 在` BPTT` 对于误差项$\delta_t,k$的计算中将其展开可以得到$\delta_{t, k}=\prod_{\tau=k}^{t-1}\left(\operatorname{diag}\left(f^{\prime}\left(\boldsymbol{z}_{\tau}\right)\right) \boldsymbol{U}^{\top}\right) \delta_{t, t}$，对角元素只要有一个小于1,那么n次乘积后会趋近于0,即当间隔比较大时,梯度变得很小，出现类似梯度消失问题;对角元素只要有一个大于1,那么n次乘积后会趋近无穷大,即当间隔比较大时,梯度变也会变得很大，出现类似梯度爆炸问题.虽然简单循环网络理论上可以建立长时间间隔的状态之间的依赖关系，但是由于梯度爆炸或消失问题，实际上只能学习到短期的依赖关系.

  解决：

  - 梯度消失：权重衰减,梯度截断

  - 梯度消失：优化模型
    $$
    h_t = h_{t-1} + g(x_t,h_{t-1};\theta)
    $$
    令$h_t$ 和$ h_{t-1}$之间存在线性和非线性的关系，但仍然存在梯度爆炸和记忆容量饱和的问题。通过引入门控机制来进一步改进模型。

  



`长短期记忆网络LSTM`是循环神经网络的一个变体，可以有效地解决简单循环神经网络的梯度爆炸或消失问题.

- 引入门控机制来控制信息传递的路径. 输入门$𝒊_𝑡$、遗忘门$𝒇_𝑡 $和输出门$𝒐_𝑡$.
- 通过`LSTM`循环单元，整个网络可以建立较长距离的时序依赖关系.记忆循环神经网络中的隐状态$h$存储了历史信息,可以看作一种记忆.在简单循环网络中,隐状态每个时刻都会被重写,因此可以看作一种短期记忆.而在`LSTM`中,记忆单元可以在某个时刻捕捉到某个关键信息,并有能力将此关键信息保存一定的时间间隔.记忆单元c中保存信息的生命周期要长于短期记忆$h$,但又远远短于长期记忆,因此称为长短期记忆(Long
- ![image-20211121223232489](/Users/jin/Library/Application Support/typora-user-images/image-20211121223232489.png)

- 对于长程依赖问题:
  $$
  \begin{gathered}
  \mathbf{C}_{t}=f_{t} \odot \mathbf{C}_{t-1}+i_{t} \odot \hat{\mathbf{C}}_{t} \\
  \mathbf{h}_{t}=o_{t} \odot \mathbf{C}_{t}
  \end{gathered}
  $$
  反向传播时有两个隐态, $\delta_{\mathbf{C}}^{(t)}=\frac{\partial J}{\partial \mathbf{C}^{(t)}}, \delta_{\mathbf{h}}^{(t)}=\frac{\partial J}{\partial \mathbf{h}^{(t)}}$, 
  $$
  \delta_{\mathbf{C}}^{(t)}=\frac{\partial J}{\partial \mathbf{C}^{(t+1)}} \frac{\partial \mathbf{C}^{(t+1)}}{\partial \mathbf{C}^{(t)}}+\frac{\partial J}{\partial \mathbf{h}^{(t)}} \frac{\partial \mathbf{h}^{(t)}}{\partial \mathbf{C}^{(t)}}=\delta_{\mathbf{C}}^{(t+1)} \odot f^{(t+1)}+\delta_{\mathbf{h}}^{(t)} \odot o^{(t)} \odot\left(1-\tanh ^{2}\left(\mathbf{C}^{(t)}\right)\right)
  $$
  起作用的是加号前的项,$f^{(t+1)}$控制着梯度衰减的程度.



`门控循环单元GRU`是一种比 `LSTM` 更加简单的循环神经网络.`GRU` 引入门控机制来控制信息更新的方式.`GRU`直接使用一个门来控制输入和遗忘之间的平衡.

![image-20211121224913639](/Users/jin/Library/Application Support/typora-user-images/image-20211121224913639.png)



`RNN`可以很容易地扩展到更广义的图结构数据上，称为`图网络`.`递归神经网络RecNN`是一种在有向无环图上的简单的图网络. 图网络是目前新兴的研究方向，还没有比较成熟的网络模型.在不同的网络结构以及任务上，都有很多不同的具体实现方式.
