

第4周
=====

[TOC]

八、神经网络：表述(Neural Networks: Representation)
-----------------------------------------------------

### 8.1 非线性假设

参考视频: 8 - 1 - Non-linear Hypotheses (10 min).mkv

​	我们之前学的，无论是**线性回归**还是**逻辑回归**都有这样一个缺点，即：当特征太多时，计算的负荷会非常大。

​	下面是一个例子：

![](../images/5316b24cd40908fb5cb1db5a055e4de5.png)

​	当我们使用$x_1$, $x_2$ 的多次项式进行预测时，我们可以应用的很好。
​	之前我们已经看到过，使用非线性的多项式项，能够帮助我们建立更好的分类模型。假设我们有非常多的特征，例如大于100个变量，我们希望用这100个特征来构建一个非线性的多项式模型，结果将是数量非常惊人的特征组合，即便我们只采用两两特征的组合$(x_1x_2+x_1x_3+x_1x_4+...+x_2x_3+x_2x_4+...+x_{99}x_{100})$，我们也会有接近5000个组合而成的特征。这对于一般的逻辑回归来说需要计算的特征太多了。

​	假设我们希望训练一个模型来识别视觉对象（例如识别一张图片上是否是一辆汽车），我们怎样才能这么做呢？一种方法是我们利用很多汽车的图片和很多非汽车的图片，然后利用这些图片上一个个像素的值（饱和度或亮度）来作为特征。

​	假如我们只选用灰度图片，每个像素则只有一个值（而非 **RGB**值），我们可以选取图片上的两个不同位置上的两个像素，然后训练一个逻辑回归算法利用这两个像素的值来判断图片上是否是汽车：

![](../images/3ac5e06e852ad3deef4cba782ebe425b.jpg)

​	假使我们采用的都是50x50像素的小图片，并且我们将所有的像素视为特征，则会有 2500个特征，如果我们要进一步将两两特征组合构成一个多项式模型，则会有约${{2500}^{2}}/2$个（接近3百万个）特征。普通的逻辑回归模型，不能有效地处理这么多的特征，这时候我们需要神经网络。

### 8.2 神经元和大脑

参考视频: 8 - 2 - Neurons and the Brain (8 min).mkv

​	神经网络**Neural Network**是一种很古老的算法，它最初产生的目的是制造能模拟大脑的算法**Algorithm that try to mimic the brain**。

​	神经网络逐渐兴起于二十世纪八九十年代，应用得非常广泛。但由于各种原因，在90年代的后期应用减少了。但是最近，神经网络又东山再起了。其中一个原因是：神经网络是计算量有些偏大的算法。然而大概由于近些年计算机的运行速度变快，才足以真正运行起大规模的神经网络。正是由于这个原因和其他一些我们后面会讨论到的技术因素，如今的神经网络对于许多应用来说是最先进的技术。

​	我们能学习数学，学着做微积分，而且大脑能处理各种不同的令人惊奇的事情。似乎如果你想要模仿它，你得写很多不同的软件来模拟所有这些五花八门的奇妙的事情。不过能不能假设大脑做所有这些，不同事情的方法，不需要用上千个不同的程序去实现。相反的，大脑处理的方法，只需要一个单一的学习算法就可以了？尽管这只是一个假设，不过让我和你分享，一些这方面的证据。

![](../images/7912ea75bc7982998870721cb1177226.jpg)

​	大脑的这一部分这一小片红色区域是你的听觉皮层，你现在正在理解我的话，这靠的是耳朵。耳朵接收到声音信号，并把声音信号传递给你的听觉皮层，正因如此，你才能明白我的话。

​	神经系统科学家做了下面这个有趣的实验，把耳朵到听觉皮层的神经切断。在这种情况下，将其重新接到一个动物的大脑上，这样从眼睛到视神经的信号最终将传到听觉皮层。如果这样做了。那么结果表明听觉皮层将会学会“看”。这里的“看”代表了我们所知道的每层含义。所以，如果你对动物这样做，那么动物就可以完成视觉辨别任务，它们可以看图像，并根据图像做出适当的决定。它们正是通过脑组织中的这个部分完成的。

​	下面再举另一个例子，这块红色的脑组织是你的躯体感觉皮层，这是你用来处理触觉的，如果你做一个和刚才类似的重接实验，那么躯体感觉皮层也能学会“看”。这个实验和其它一些类似的实验，被称为神经重接实验，从这个意义上说，如果人体有同一块脑组织可以处理光、声或触觉信号，那么也许存在一种学习算法，可以同时处理视觉、听觉和触觉，而不是需要运行上千个不同的程序，或者上千个不同的算法来做这些大脑所完成的成千上万的美好事情。也许我们需要做的就是找出一些近似的或实际的大脑学习算法，然后实现它大脑通过自学掌握如何处理这些不同类型的数据。在很大的程度上，可以猜想如果我们把几乎任何一种传感器接入到大脑的几乎任何一个部位的话，大脑就会学会处理它。

​	下面再举几个例子：

![](../images/2b74c1eeff95db47f5ebd8aef1290f09.jpg)

​	这张图是用舌头学会“看”的一个例子。它的原理是：这实际上是一个名为**BrainPort**的系统，它现在正在**FDA**(美国食品和药物管理局)的临床试验阶段，它能帮助失明人士看见事物。它的原理是，你在前额上带一个灰度摄像头，面朝前，它就能获取你面前事物的低分辨率的灰度图像。你连一根线到舌头上安装的电极阵列上，那么每个像素都被映射到你舌头的某个位置上，可能电压值高的点对应一个暗像素电压值低的点。对应于亮像素，即使依靠它现在的功能，使用这种系统就能让你我在几十分钟里就学会用我们的舌头“看”东西。

![](../images/95c020b2227ca4b9a9bcbd40099d1766.png)

​	这是第二个例子，关于人体回声定位或者说人体声纳。你有两种方法可以实现：你可以弹响指，或者咂舌头。不过现在有失明人士，确实在学校里接受这样的培训，并学会解读从环境反弹回来的声波模式—这就是声纳。如果你搜索**YouTube**之后，就会发现有些视频讲述了一个令人称奇的孩子，他因为癌症眼球惨遭移除，虽然失去了眼球，但是通过打响指，他可以四处走动而不撞到任何东西，他能滑滑板，他可以将篮球投入篮框中。注意这是一个没有眼球的孩子。

![](../images/697ae58b1370e81749f9feb333bdf842.png)

​	第三个例子是触觉皮带，如果你把它戴在腰上，蜂鸣器会响，而且总是朝向北时发出嗡嗡声。它可以使人拥有方向感，用类似于鸟类感知方向的方式。

​	还有一些离奇的例子：

![](../images/1ee5c76a62b35384491c603bb54c8c0c.png)

​	如果你在青蛙身上插入第三只眼，青蛙也能学会使用那只眼睛。因此，这将会非常令人惊奇。如果你能把几乎任何传感器接入到大脑中，大脑的学习算法就能找出学习数据的方法，并处理这些数据。从某种意义上来说，如果我们能找出大脑的学习算法，然后在计算机上执行大脑学习算法或与之相似的算法，也许这将是我们向人工智能迈进做出的最好的尝试。人工智能的梦想就是：有一天能制造出真正的智能机器。

​	神经网络可能为我们打开一扇进入遥远的人工智能梦的窗户，但我在这节课中讲授神经网络的原因，主要是对于现代机器学习应用。它是最有效的技术方法。因此在接下来的一些课程中，我们将开始深入到神经网络的技术细节。

### 8.3 模型表示1

参考视频: 8 - 3 - Model Representation I (12 min).mkv

​	为了构建神经网络模型，我们需要首先思考大脑中的神经网络是怎样的？每一个神经元都可以被认为是一个处理单元/神经核**processing unit**/**Nucleus**，它含有许多输入/树突**input**/**Dendrite**，并且有一个输出/轴突**output**/**Axon**。神经网络是大量神经元相互链接并通过电脉冲来交流的一个网络。

![](../images/3d93e8c1cd681c2b3599f05739e3f3cc.jpg)

​	下面是一组神经元的示意图，神经元利用微弱的电流进行沟通。这些弱电流也称作动作电位，其实就是一些微弱的电流。所以如果神经元想要传递一个消息，它就会就通过它的轴突，发送一段微弱电流给其他神经元，这就是轴突。

​	这里是一条连接到输入神经，或者连接另一个神经元树突的神经，接下来这个神经元接收这条消息，做一些计算，它有可能会反过来将在轴突上的自己的消息传给其他神经元。这就是所有人类思考的模型：我们的神经元把自己的收到的消息进行计算，并向其他神经元传递消息。这也是我们的感觉和肌肉运转的原理。如果你想活动一块肌肉，就会触发一个神经元给你的肌肉发送脉冲，并引起你的肌肉收缩。如果一些感官：比如说眼睛想要给大脑传递一个消息，那么它就像这样发送电脉冲给大脑的。

![image-20210530232843810](week4.assets/image-20210530232843810.png)

​	神经网络模型建立在很多神经元之上，每一个神经元又是一个个学习模型。这些神经元（也叫激活单元，**activation unit**）采纳一些特征作为输出，并且根据本身的模型提供一个输出。

<img src="week4.assets/image-20210530234001285.png" alt="image-20210530234001285" style="zoom:50%;" />

​	如上图所示，黄色圆圈可以理解为神经元的东西，然后我们通过，它的树突或者说它的输入神经，传递给它一些信息，然后神经元做一些计算，并通过它的输出神经，即它的轴突，输出计算结果。这里的$h_\theta(x) = \frac{1}{1+e^{-\theta^Tx}}$。通常x和θ是我们的参数向量，这是一个简单的模型，它被输入$x_1$，$x_2$和$x_3$，然后输出一些类似这样的结果。

​	绘制一个神经网络时，通常只绘制输入节点$x_1$，$x_2$,$x_3$，但有时也可以增加一个额外的节点$x_0$，这个$x_0$节点有时也被称作偏置单位，或==偏置神经元**bias neuron**，但因为$x_0$总是等于1==，所以有时候会画出它有时不会画出，这取决于它是否对例子有利。

​	有时我们会说，这是一个神经元，一个有s型函数或者逻辑函数作为激励函数**activetion function**的人工神经元。

​	在神经网络术语中，==激励函数只是对类似非线性函数$g(z)$的另一个术语称呼==，$g(z) = \frac{1}{1+e^{-z}}$。到目前为止，我一直称θ为模型的参数，以后大概会继续将这个术语与“参数”相对应。

​	在神经网络中，参数**parameters**又可被成为权重**weight**，是一样的东西。

​	下图中的这些小圈，代表一个单一的神经元，神经网络其实就是，这些不同的神经元，组合在一起的集合。

![](../images/fbb4ffb48b64468c384647d45f7b86b5.png)

- $x_1$, $x_2$, $x_3$是**输入单元**，我们将原始数据输入给它们。
- $a_1$, $a_2$, $a_3$是**中间单元**，它们负责将数据进行处理，然后呈递到下一层。
- $Layer3$中最后是**输出单元**，它负责计算${h_\theta}\left( x \right)$。

​	神经网络模型是许多逻辑单元按照不同层级组织起来的网络，每一层的输出变量都是下一层的输入变量。

​	下图为一个3层的神经网络，

- 第一层成为输入层**Input Layer**
- 最后一层称为输出层**Output Layer**
- 中间一层成为隐藏层**Hidden Layers**

<img src="week4.assets/image-20210530235303091.png" alt="image-20210530235303091" style="zoom:50%;" />

​	下面引入一些标记法来帮助描述模型：

- $a_{i}^{\left( j \right)}$ 代表第$j$ 层的第 $i$ 个**激活单元**。**"activation " of unit  $i$ in layer $j$**。

  图中比如$a_{1}^{(2)}$，表示第2层的第一个激励，即隐藏层的第一个激励，所谓==激励**activation**，是指由一个具体神经元读入，计算并输出的值==。

- ${{\theta }^{\left( j \right)}}$代表从第 $j$ 层映射到第$ j+1$ 层时的权重的矩阵，它将成为一个波矩阵**matrix of waves**。例如${{\theta }^{\left( 1 \right)}}$代表从第一层映射到第二层的权重的矩阵。

  其尺寸为：==以第 $j+1$层的激活单元数量为行数，以第 $j$ 层的激活单元数加一为列数的矩阵==。例如：上图所示的神经网络中${{\theta }^{\left( 1 \right)}}$的尺寸为 3*4。

  - 如果一个网络在第$j$层有$s_j$个单元，在$j+1$层有$s_{j+1}$个单元，那么矩阵$\Theta^(j)$，即控制第$j$层到第$j+1$层映射的矩阵的，维度为

$$
s_{j+1}×(s_j+1)
$$

​				所以$\Theta^{(j)}$的维度是，$s_{j+1}$行，$s_j+1$列。   



​	对于上图所示的模型，$Layer2$激活单元和$Layer3$输出分别表达为：
$$
\large {
a_{1}^{(2)}=g(\Theta _{10}^{(1)}{{x}_{0}}+\Theta _{11}^{(1)}{{x}_{1}}+\Theta _{12}^{(1)}{{x}_{2}}+\Theta _{13}^{(1)}{{x}_{3}})\\
a_{2}^{(2)}=g(\Theta _{20}^{(1)}{{x}_{0}}+\Theta _{21}^{(1)}{{x}_{1}}+\Theta _{22}^{(1)}{{x}_{2}}+\Theta _{23}^{(1)}{{x}_{3}})\\
a_{3}^{(2)}=g(\Theta _{30}^{(1)}{{x}_{0}}+\Theta _{31}^{(1)}{{x}_{1}}+\Theta _{32}^{(1)}{{x}_{2}}+\Theta _{33}^{(1)}{{x}_{3}})\\
{{h}_{\Theta }}(x)=g(\Theta _{10}^{(2)}a_{0}^{(2)}+\Theta _{11}^{(2)}a_{1}^{(2)}+\Theta _{12}^{(2)}a_{2}^{(2)}+\Theta _{13}^{(2)}a_{3}^{(2)})
}
$$

1. 第一个公式中输入的线性组合上的结果

2. 第二个隐藏单元，等于$s$函数作用在这个第二个线性组合上的值，以此类推。

3. 最后，在输出层，还有一个单元，它计算$h(x)$ 

   注意我写了$\Theta ^{(2)}$的形式，这是参数矩阵\权重矩阵，该矩阵控制从隐藏层的3个单位到第三层的一个单元，即输出单元的映射。

​    总之，以上我们，展示了像这样一张图是怎样定义一个人工神经网络的，这个神经网络定义了函数$h$从输入$x$到输出$y$的映射，我将这些假设的参数，记为大写的$\Theta$，这样一来，不同的$\Theta$，对应了不同的假设，所以我们有不同的函数，比如说从，x到y的映射。



### 8.4 模型表示2

参考视频: 8 - 4 - Model Representation II (12 min).mkv

<img src="week4.assets/image-20210530235303091.png" alt="image-20210530235303091" style="zoom: 50%;" />

​	在上述的式子里，我们可以将$g$的部分表达为$z_1^{(2)}$

​	即:
$$
a_1^2 = g(z_1^{(2)})\\a_2^2 = g(z_2^{(2)})\\a_3^2 = g(z_3^{(2)})
$$
​	所以这些z值都是一个线性组合，是输入值$x_0$,$x_1$,$x_2$,$x_3$的加权线性组合，他将会进入一个特定的神经元。

​	相对于使用循环来编码，利用向量化的方法会使得计算更为简便。以上面的神经网络为例。

1. 计算第二层的值：

$$
x=\left[\begin{array}{c}
x_{0} \\
x_{1} \\
x_{2} \\
x_{3}
\end{array}\right] z^{(2)}=\left[\begin{array}{c}
z_{1}^{(2)} \\
z_{2}^{(2)} \\
z_{3}^{(2)}
\end{array}\right]
$$

$$
\begin{array}{l}
a^{(2)}=g\left(z^{(2)}\right)\\
z^{(2)}=\Theta^{(1)} x 
\end{array}
$$

​		公式推导可得：
$$
\small{
a^{(2)}\\
=g(\Theta^{(1)} x )\\
=g\left(\left[\begin{array}{llll}
\Theta_{10}^{(1)} & \Theta_{11}^{(1)} & \Theta_{12}^{(1)} & \Theta_{13}^{(1)} \\
\Theta_{20}^{(1)} & \Theta_{21}^{(1)} & \Theta_{22}^{(1)} & \Theta_{23}^{(1)} \\
\Theta_{30}^{(1)} & \Theta_{31}^{(1)} & \Theta_{32}^{(1)} & \Theta_{33}^{(1)}
\end{array}\right] \times\left[\begin{array}{c}
x_{0} \\
x_{1} \\
x_{2} \\
x_{3}
\end{array}\right]\right)\\
=g\left(\left[\begin{array}{l}
\Theta_{10}^{(1)} x_{0}+\Theta_{11}^{(1)} x_{1}+\Theta_{12}^{(1)} x_{2}+\Theta_{13}^{(1)} x_{3} \\
\Theta_{20}^{(1)} x_{0}+\Theta_{21}^{(1)} x_{1}+\Theta_{22}^{(1)} x_{2}+\Theta_{23}^{(1)} x_{3} \\
\Theta_{30}^{(1)} x_{0}+\Theta_{31}^{(1)} x_{1}+\Theta_{32}^{(1)} x_{2}+\Theta_{33}^{(1)} x_{3}
\end{array}\right]\right)\\
=g\left(\left[\begin{array}{c}
z_{1}^{(2)} \\
z_{2}^{(2)} \\
z_{3}^{(2)}
\end{array}\right]\right) \\
=g\left(z^{(2)} \right) \\
=\left[\begin{array}{l}
a_{1}^{(2)} \\
a_{2}^{(2)} \\
a_{3}^{(2)}
\end{array}\right]
}
$$
​	这里的$z^{(2)}$是个三维向量，$a^{(2)}$也是个三维向量，因此这里的Sigmod函数逐元素作用于$z^{(2)}$中的每个元素。

​	
$$
z^{(2)}=\Theta^{(1)} a^{(1)}
$$
​	这里我们还有隐藏的偏置单元只是未在图中画出，我们额外加上一个$a_0^{(2)} = 1$,这样$a^{(2)}$就是一个四维特征向量。
$$
a^{(2)}=\left[\begin{array}{c}
1 \\
g\left(z_1^{(2)}\right) \\
g\left(z_2^{(2)}\right) \\
g\left(z_3^{(2)}\right) 
\end{array}\right]
$$

- 进行推广

  - 定义$x = a^{(1)}$,我们可以将等式转换成：

  $$
  z^{(j)}=\Theta^{(j-1)} a^{(j-1)}
  $$

  - 当对于$j=2$的第$k$个节点有：

  $$
  z_{k}^{(2)}=\Theta_{k, 0}^{(1)} x_{0}+\Theta_{k, 1}^{(1)} x_{1}+\cdots+\Theta_{k, n}^{(1)} x_{n}
  $$

  - 用向量表示$x$和$z^j$为：

$$
x=\left[\begin{array}{c}
x_{0} \\
x_{1} \\
\cdots \\
x_{n}
\end{array}\right] z^{(j)}=\left[\begin{array}{c}
z_{1}^{(j)} \\
z_{2}^{(j)} \\
\cdots \\
z_{n}^{(j)}
\end{array}\right]
$$

$$
\begin{array}{l}
a^{(2)}=g\left(z^{(2)}\right)\\
z^{(2)}=\Theta^{(1)} x 
\end{array}
$$



2. 计算假设的实际输出值，我们只需要计算$z^{(3)}$

$$
z^{(3)} = \Theta^{(2)}a^{(2)}\\
a^{(3)}=g\left(z^{(3)}\right)\\
$$

​		则假设输出	
$$
\small{
a^{(3)}\\
=g(\Theta^{(2)}a^{(2)})\\
=g\left(\left[\begin{array}{llll}
\Theta_{10}^{(2)} & \Theta_{11}^{(2)} & \Theta_{12}^{(2)} & \Theta_{13}^{(2)}
\end{array}\right] \times\left[\begin{array}{c}
a_{0}^{(2)} \\
a_{1}^{(2)} \\
a_{2}^{(2)} \\
a_{2}^{(2)}
\end{array}\right ]\right)\\
=g\left(\Theta_{10}^{(2)} a_{0}^{(2)}+\Theta_{11}^{(2)} a_{1}^{(2)}+\Theta_{12}^{(2)} a_{2}^{(2)}+\Theta_{13}^{(2)} a_{3}^{(2)}\right)\\
=h_{\Theta}(x)
}
$$


​		即：
$$
h_\Theta(x) = a^{(3)} = g(z^{(3)})
$$
![image-20210531115939979](week4.assets/image-20210531115939979.png)	

​	这个计算$h(x)$的过程，也称为前向传播**forward propagation**，这样命名是因为我们从输入层的激励开始，然后前向传播给隐藏层并计算隐藏层的激励，然后我们继续前向传播，并计算输出层的激励。这个==从输入层到隐藏层再到输出层依次计算激励的过程就叫做前向传播==。

 

- 进行推广

  ​	这只是针对训练集中一个训练实例所进行的计算。如果我们要对整个训练集进行计算，我们需要将训练集特征矩阵进行转置，使得同一个实例的特征都在同一列里。即：
  $$
  {{z}^{\left( 2 \right)}}={{\Theta }^{\left( 1 \right)}}\times {{X}^{T}} \\
  
   {{a}^{\left( 2 \right)}}=g({{z}^{\left( 2 \right)}})
  $$

### 8.5 模型表示3

​	为了更好的了解**Neuron Networks**的工作原理，我们先把左半部分遮住：

![](../images/6167ad04e696c400cb9e1b7dc1e58d8a.png)

​	右半部分其实就是以$a_0, a_1, a_2, a_3$, 按照**Logistic Regression**的方式输出$h_\theta(x)$：
$$
\large
{{h}_{\Theta }}(x)=g(\Theta _{1,0}^{(2)}a_{0}^{(2)}+\Theta _{1,1}^{(2)}a_{1}^{(2)}+\Theta _{1,2}^{(2)}a_{2}^{(2)}+\Theta _{1,3}^{(2)}a_{3}^{(2)})
$$
​	其实神经网络就像是**logistic regression**，只不过我们把逻辑回归中的输入向量$\left[ x_1\sim {x_3} \right]$ 变成了中间层的$\left[ a_1^{(2)}\sim a_3^{(2)} \right]$, 即:
$$
h_\theta(x)=g\left( \Theta_0^{\left( 2 \right)}a_0^{\left( 2 \right)}+\Theta_1^{\left( 2 \right)}a_1^{\left( 2 \right)}+\Theta_{2}^{\left( 2 \right)}a_{2}^{\left( 2 \right)}+\Theta_{3}^{\left( 2 \right)}a_{3}^{\left( 2 \right)} \right)
$$
​	我们可以把$a_0, a_1, a_2, a_3$看成更为高级的特征值，也就是$x_0, x_1, x_2, x_3$的进化体，并且它们是由 $x$与$\theta$决定的，因为是梯度下降的，所以$a$是变化的，并且变得越来越厉害，所以这些更高级的特征值远比仅仅将 $x$次方厉害，也能更好的预测新数据。
​	这就是神经网络相比于逻辑回归和线性回归的优势。



​	你还可以用其他类型的图来表示神经网络中神经元相连接的方式，称为神经网络的架构，所以说，==架构**architecture**是指，不同的神经元是如何相互连接的==。

​	神经网络架构的例子如下图所示：

<img src="week4.assets/image-20210531125658854.png" alt="image-20210531125658854" style="zoom: 50%;" />

​	你可以，意识到这个第二层是如何工作的。在这里，我们有三个隐藏单元，它们根据输入层计算一个复杂的函数，然后第三层可以将第二层训练出的特征项作为输入，并在第三层计算一些更复杂的函数，这样，在你到达输出层之前即第四层，就可以利用第三层训练出的更复杂的特征项作为输入，以此得到非常有趣的非线性假设。

​	==每一个$a$都是由上一层所有的$x$和每一个$x$所对应的决定的==

​	顺便说一下，在这样的网络里，第一层被称为输入层，第四层仍然是我们的输出层，这个网络有两个隐藏层，所以，任何一个不是，输入层或输出层的，都被称为隐藏层。






### 8.6 特征和直观理解1

参考视频: 8 - 5 - Examples and Intuitions I (7 min).mkv

​	从本质上讲，神经网络能够通过学习得出其自身的一系列特征。在普通的逻辑回归中，我们被限制为使用数据中的原始特征$x_1,x_2,...,{{x}_{n}}$，我们虽然可以使用一些二项式项来组合这些特征，但是我们仍然受到这些原始特征的限制。

​	在神经网络中，原始特征只是输入层，在我们上面三层的神经网络例子中，第三层也就是输出层做出的预测利用的是第二层的特征，而非输入层中的原始特征，我们可以认为第二层中的特征是神经网络通过学习后自己得出的一系列用于预测输出变量的新特征。

​	 神经网络中，单层神经元（无中间层）的计算可用来表示逻辑运算，比如逻辑与(**AND**)、逻辑或(**OR**)。

- 逻辑与**AND**

  ​	下图是神经网络的设计与**output**层表达式，我们可以用这样的一个神经网络表示**AND** 函数：

<img src="week4.assets/image-20210531150538553.png" alt="image-20210531150538553" style="zoom:50%;" />

​			其中$\Theta_0 = -30, \Theta_1 = 20, \Theta_2 = 20$

​			我们的输出函数$h_\theta(x)$即为：
$$
h_\Theta(x)=g\left(\Theta_{10}^{(1)}+\Theta_{11}^{(1)}+\Theta_{12}^{(1)} \right)=g\left( -30+20x_1+20x_2 \right)
$$
​			我们知道$g(x)$的图像是：

<img src="week4.assets/image-20210531151018370.png" alt="image-20210531151018370" style="zoom: 50%;" />

​			真值表如下所示：

![](../images/f75115da9090701516aa1ff0295436dd.png)

​			所以我们有：$h_\Theta(x) \approx {x}_1 \text{AND} \, {x}_2$



- 逻辑或**OR**

  ![](../images/aa27671f7a3a16545a28f356a2fb98c0.png)

  **OR**与**AND**整体一样，区别只在于的取值不同。

  

- 逻辑非**NOT**![](week4.assets/1fd3017dfa554642a5e1805d6d2b1fa6.png)

  ​	通过观察我们可以知道：在你希望取非运算的变量前面，放上一个绝对值大的负数作为权值。





### 8.7 样本和直观理解II

参考视频: 8 - 6 - Examples and Intuitions II (10 min).mkv

​	二元逻辑运算符**BINARY LOGICAL OPERATORS**当输入特征为布尔值（0或1）时，我们可以用一个单一的激活层可以作为二元逻辑运算符，为了表示不同的运算符，我们只需要选择不同的权重即可。

​	我们可以利用神经元来组合成更为复杂的神经网络以实现更复杂的运算。

- 逻辑异或**XOR**：
  $$
  \text{XOR}= \left( \text{NOT}\, \text{x}_1 \right) \text{AND} \left( \text{NOT}\, \text{x}_2 \right)
  $$
  <img src="../images/4c44e69a12b48efdff2fe92a0a698768.png" style="zoom:75%;" />

- 逻辑异或非**XNOR**
  $$
  \text{XNOR}=( \text{x}_1\, \text{AND}\, \text{x}_2 )\, \text{OR} \left( \left( \text{NOT}\, \text{x}_1 \right) \text{AND} \left( \text{NOT}\, \text{x}_2 \right) \right)
  $$
  ​	首先构造一个能表达$\left( \text{NOT}\, \text{x}_1 \right) \text{AND} \left( \text{NOT}\, \text{x}_2 \right)$部分的神经元，如上图所示。

  ​	然后将表示 **AND** 的神经元和表示$\left( \text{NOT}\, \text{x}_1 \right) \text{AND} \left( \text{NOT}\, \text{x}_2 \right)$的神经元以及表示 OR 的神经元进行组合：

  ![image-20210531163304108](week4.assets/image-20210531163304108.png)

​			我们就得到了一个能实现 $\text{XNOR}$ 运算符功能的神经网络。

​	按这种方法我们可以逐渐构造出越来越复杂的函数，也能得到更加厉害的特征值。

### 8.8 多类分类

参考视频: 8 - 7 - Multiclass Classification (4 min).mkv

​	当我们有不止两种分类时（也就是$y=1,2,3….$），比如以下这种情况，该怎么办？如果我们要训练一个神经网络算法来识别路人、汽车、摩托车和卡车，在输出层我们应该有4个值。例如，第一个值为1或0用于预测是否是行人，第二个值用于判断是否为汽车。

​	输入向量$x$有三个维度，两个中间层，输出层4个神经元分别用来表示4类，也就是每一个数据在输出层都会出现${{\left[ a\text{ }b\text{ }c\text{ }d \right]}^{T}}$，且$a,b,c,d$中仅有一个为1，表示当前类。

​	下面是该神经网络的可能结构示例：

![image-20210531164839276](week4.assets/image-20210531164839276.png)

​	神经网络算法的输出结果为四种可能情形之一：

![](../images/5e1a39d165f272b7f145c68ef78a3e13.png)